Again it's been a while since the last update!  The last time I worked on the project (or anything, really) was about two weeks ago when I made some changes to the project for the sake of giving a presentation about ORMs for my day job.  I examined the potential use of Subsonic (ActiveRecord and Simple Repository), DapperORM, and of course the already existing Entity Framework "Code First" approach.

I also made several other miscellaneous changes that I'd been meaning to make and hadn't bothered to until now.

We have a lot to talk about: let's get started!

<h3>Outcome of SoapUI Poll</h3>

Admittedly, by the time Saturday came around I was feeling pretty guilty for whining about SoapUI.  It works, and the drawbacks (need for stored procedures and such) is really not justification for throwing it out.  One of the two votes in favor of using SoapUI is actually my own as a result.  I'll get back to working with it as soon as I can get out of this slump I'm in.

<h3>Addressing Miscellaneous Issues</h3>

There were several things I needed to correct in the model that I'd been overlooking:

<ul>
<li>Support for optimistic concurrency</li>
<li>Inheritance of common properties</li>
<li>GUID Generation</li>
<li>The fate of "generated" Entry Links</li>
<li>Properties that should be re-examined</li>
</ul>

The first two I resolved in one fell swoop: the latter issues are still ongoing, but I'll discuss them here.

<h3>Optimistic Concurrency and Common Properties</h3>

Optimistic Concurrency is the concept of using some form of stateful information about a persisted entity to determine whether it has been changed since the last time the current user has acquired its state.  It's useful to prevent situations such as when one user modified an Entity while another user is still working on a change.  Without optimistic concurrency checks, the second user's changes could overwrite the first user's changes without ever knowing that changes had even been made.

There's several ways to approach optimistic concurrency.  For starters, when you go to update an entity you could compare the current state of all of the properties of the entity in the database vs. some stored "original state" that represents what you had originally pulled before you started making changes.  

While the last solution is feasible, it is often a fair tradeoff to have a "timestamp" property for entities instead that is automatically regenerated by the database every time the entity changes.  You then have only one piece of information to compare, and only one to store the original state of: the tradeoff being that your domain entities will have a bit of "database-centric" information that isn't really relevant to the entity itself but rather to the notions of its persistence.

I'm interested in taking the timestamp approach: all first-class Entities need this timestamp for it to work, and as such this also presented a good time to create a base class for these Entities.

<pre>
public abstract class DomainEntity
{
	public int Id { get; set; }

	public byte[] VersionStamp { get; set; }

}
</pre>
(Figure 1: Our new base class for Entities)

<h3>Other Model Issues</h3>

The other pending issues around the model are ones I'll need to think more about when I'm feeling less burned out.  This commute is wearing me out, I'm definitely having a hard time thinking about side projects of any sort.  But I digress.

The biggest still-pending question regards entry links.  In the standard, most "entry links" were generated or can be implied by other properties of the entity.  I need to re-read the specs and see if, in fact, they all are: or whether there are custom entry links that can be created.  I may have taken the wrong approach with the concept of links, and I need to re-address it.

For that matter, I need to make properties like Entry.SourceUri, Collections.Href, etc... transient properties.  They ought to be generated based on the contextual information in the Entities themselves, and not stored separately.  At least for now.  I think I made these persistable in anticipation of potentially needing them to be for Federated servers, but I think I was getting too far ahead of myself.  I must solve the problems that need to be solved for this iteration, and reapproach when the new features arrive.

I also need to re-address the notion of Content Types.  I have dealt with it inconsistently up to this point: I made a separate table and relationship for these, yet the Entry entity has a text value for its content type while the Feed has a many-to-many relationship with that content type table.  I think that I need to make a choice about strict vs. free-form content types and stick with it.

The other big issue is how I ought to handle both the creation and storage of media entries.  I've been ignoring this type of entity for the most part so far, but we're at the point where I need to start paying attention to it and its potential effect on what the model ought to look like.  The workflow for their creation is notably different than text-based Entries, and may actually warrant an entirely new Entry concept or a rethinking of the original Entry model.

So these are the still pending issues.  Let's take a look at the changes I made to support additional repositories.

<h3>Multiple Repository Support</h3>

In order to support new Repositories, I first had to make a more clear separation between the Domain Model and the Repositories that I'd already created.

I started by examining what actually constituted the realm of persistence.  The Factories and Models I had created were - thanks to the relatively clear separation of model from persistence - obviously the core components to keep in the Domain project itself.

I still had some work to do, however.  Currently, I was using a hacked-together set of Repositories that were mostly generated from the experiments with MvcScaffolding.  Repositories exposed an interface, but that interface was still in the same physical file as the implemented Repository itself.  I extracted out those interfaces, and identifying commonalities I created an IRepository interface that individual Repositories would be based off of.

I also needed an easy way to switch which Persistence library I should use for injection.  I created a simple interface - IPersistenceInjector - that expects a method called "RegisterRepositories" to be implemented.  Passing in an AutoFac ContainerBuilder, it should register IRepository instances for each of the expected Repositories.  (I also added a test method to the interface, because I was feeling particularly lazy)  When, eventually, I switch to initializing dependencies through configuration files, this won't be necessary: but for now, it was an easy way to switch the entire persistence mechanism with a single line of code.

At this point, each persistence mechanism simply has to implement an IRepository of each expected type (by any means necessary, as you'll soon see), and an IPersistenceInjector.  Theoretically, it's done at that point!

<h3>It's never <i>quite</i> that easy...</h3>

Unfortunately, during the process of examining the various other ORMs I realized that there were certain differences that I was either going to have to build a lot more infrastructure than I wanted to, or to make some sacrifices for the academic example purposes.  I chose the latter.

The crux of the problem is that the Entity Framework navitely supports a concept similar to a Unit of Work with its Context class: updates, deletes, and creation are all delayed until the Save() method is called, at which point the requested operations are executed as a single unit.  Unfortunately for ease of compatibility, Subsonic and Dapper don't have a similar concept by default: you must explicitly state your intent to Update a given entity - for example - and such updates, deletions, etc... all happen as soon as they're requested (or at least are added to a transaction if you're using one).

My hackaround to avoid having to build support for a Context-like notion for Dapper and Subsonic was to slightly change the way that the Repository interfaces worked: instead of implicit Updates, I added a method to the interface for explicit updates.  This method - for the Entity Framework - was empty, whereas it performed actions for the other Repositories.  The other repositories, in turn, didn't do anything in response to the Save() method.

<pre>
[HttpPost]
public ActionResult Create(Collection d)
{
	if (ModelState.IsValid)
	{
	  this._repository.Add(d);
	  this._repository.Save();
	  return RedirectToAction("Index");  
	}
	return View();
}
</pre>
(Figure 2: Hax!)

This was most certainly a hack: the repository implementations do not behave the same in practice, as with EF none of your CUD operations persist immediately while they do with the others.  However, for the academic purposes it worked well enough.

Once that was done, I was ready to create new Repositories.

<h3>Subsonic SimpleRepository</h3>

Last year when I had my brief love affair with Subsonic, I had heard about SimpleRepository: Subsonic's built in alternative to using their Linq-friendly ActiveRecord generated ORM.

For those who have never used Subsonic, I encourage you to try it out on a small future project.  It's beginning to show its age in the face of new ORM approaches like Entity Framework Code First(it already hadn't been updated for several months back when I was first introduced to it in early 2010: as far as I know, it hasn't been worked on since), but for a project built on an existing database (or one where perhaps the persistence schema is more obvious than the domain, or the domain itself is simple enough that it doesn't need much abstraction from the persistence mechanism), Subsonic is fast like lightning to develop.  It also was the first library I used that made me really love Linq.  Find an existing database schema or make a new one, and try generating some ActiveRecord classes from it with Subsonic: <a href="http://subsonicproject.com/docs/The_5_Minute_Demo">this page will show you how</a>.

But I digress.  Since we already have POCO domain entities, it doesn't make much sense to generate a whole new set of them just for this implementation and implement the infrastructure such that it can retrofit into it... so I decided to give SimpleRepository a try instead.

Unlike Subsonic's recommended template generation approach, SimpleRepository is essentially a class provided by the Subsonic library that lets you add, delete, and execute Linq queries using generics and a heavy amount of database structure by convention.

You can see how, for simple scenarios, you don't have to define anything to make it "just work": Add, Remove, and basic select methods are provided and inferred by the type used.  The tradeoff for this is that you don't have any say over the structure of the resulting database: I had to create a separate database for this implementation that Subsonic could generate on the fly to comply with what it expected to find.

<pre>
public void Add(Category domainEntity)
{
	this.Repository.Add<Category>(domainEntity);
}

public void Delete(int id)
{
	this.Repository.Delete<Category>(id);
}

public IEnumerable<Category> GetAll()
{
	return this.Repository.All<Category>();
}

public Category GetById(int id)
{
	return this.Repository.Single<Category>(id);
}
</pre>
(Figure 3: Subsonic doing basic, built-in operations)

Linq queries are just as intuitive, and again work with the generous use of generics:

<pre>
(from ct in this.Repository.All<ContentType>()
join cct in this.Repository.All<CollectionContentType>() on ct.Id equals cct.ContentTypeId
where
	 cct.CollectionId == result.Id
select ct).ToList();
</pre>
(Figure 4: Get your Linq on with Subsonic: in this case, grabbing Content Types for a Collection)

Life's not all sunshine and lollipops in the land of Subsonic SimpleRepository, however.  Unlike EFCodeFirst or the templated Subsonic approach, it has no notion of table relations.  If an entity is an aggregate of another, you have to establish, populate, and manage that relationship manually:

<pre>
public void Add(Collection domainEntity)
{
	this.Repository.Add<Collection>(domainEntity);

	foreach (Category category in domainEntity.Categories)
	{
		if (category.Id == 0)
		{
			this.Repository.Add<Category>(category);
		}

		this.Repository.Add<CollectionCategory>(new CollectionCategory() { CategoryId = category.Id, CollectionId = domainEntity.Id });
	}

	foreach (ContentType contentType in domainEntity.AcceptedTypes)
	{
		if (contentType.Id == 0)
		{
			this.Repository.Add<ContentType>(contentType);
		}

		this.Repository.Add<CollectionContentType>(new CollectionContentType() { ContentTypeId = contentType.Id, CollectionId = domainEntity.Id });
	}
}
</pre>
(Figure 5: Naively manually adding aggregated children for a Collection)

This makes the repository code a bit more tedious, and also subjects you to the possibility of subtle bugs: neither the database nor the ORM is going to enforce any of your relationships, so if you miss one you will only run into it with sufficient (automated or manual) testing.

Subsonic also has no notion of a Context in the way that Entity Framework has: it relies on the explicit creation of Transactions to manage when multiple actions should occur together, and commands to insert, delete, or update happen instantaneously on the database itself.  In some cases, this can be somewhat freeing: particularly in simple applications with CRUD requirements, or where you're willing to take the time to build your own concept of a Unit of Work.

I was pressed for time, and as you can see in the code I didn't bother to create my own analogy to the Context object: instead having the "Save" method exposed by the Repositories do nothing.  That'd be a dangerous move if one was relying on the difference between CRUD operations and the Save operation, but for the sake of time and this academic exercise I let it slide.

Okay, on to Dapper!

<h3>Dapper</h3>

Dapper is a MicroORM: essentially, it provides a user friendly and fast wrapper for basic database access.  With Dapper you make raw Sql queries and let Dapper's mapping mechanisms perform the task of assigning the output to individual or collective result sets.  Dapper does this by providing extension methods on ADO.NET' Connection object.  When you include the dapper namespace, you'll have access to several new and somewhat intelligent query methods on connections (connections which you must make and manage yourself).

When you look at the code, you may ask yourself "Am I really getting anything beyond just using ADO.NET with this?"  You are, but not much more than that.  It simplifies (and in some cases, greatly simplifies) the mapping code that you'd have to write using standard ADO.NET, but you're on your own for any features beyond SELECT statements.  Maintenance of relations, the concept of unit of work, and in this case even optimistic concurrency aren't things you're going to get and you'll have to code for it yourself.

Why use it at all, you might ask?  Well, in using it to make these Repositories I came to the conclusion that it's not really fitting for a heavyweight Repository approach: and I imagine that its creators would agree.  Where it shines is in its ability to remove some of the mapping hassle while still providing you access that is essentially as fast as raw ADO.NET access.  This increased read performance can be extremely important for high-traffic sites.  In a way, using Dapper for expensive or frequently run query operations is the ORM equivalent of denormalizing for speed.

That being said, let's take a look at a simple example of a select query with Dapper:

<pre>
public Category GetById(int id)
{
	string query = @"
		SELECT
			*
		FROM
			Categories
		WHERE
			ID = @ID";

	return _context.Connection.Query<Category>(query, new { ID = id }).FirstOrDefault();
}
</pre>
(Figure 6: Dapper is dapper for simple queries.)

You pass in the parameterized SQL, and create a dynamic object as the second parameter that maps the values into the query.  Generics are, again, used here for the implicit mapping back to the resultant domain object.

For scenarios such as aggregation, we have to get a bit fancier:

<pre>
public IEnumerable<Collection> GetAll()
{

	// An example of querying for and using a single joined query.
	string query = @"
		SELECT 
			Collections.*,
			ContentTypes.*,
			Categories.*
		FROM 
			Collections
			LEFT OUTER JOIN CollectionContentTypes
				ON CollectionContentTypes.Collection_Id = Collections.Id
			LEFT OUTER JOIN ContentTypes 
				ON ContentTypes.Id = CollectionContentTypes.ContentType_Id
			LEFT OUTER JOIN CollectionCategories
				ON CollectionCategories.Collection_Id = Collections.Id
			LEFT OUTER JOIN Categories 
				ON Categories.Id = CollectionCategories.Category_Id";

	Dictionary<int, Collection> collections = new Dictionary<int, Collection>();

	return _context.Connection.Query<Collection, ContentType, Category, Collection>(query,
		(collection, contentType, category) => 
		{
			Collection existing = null;

			if (!collections.TryGetValue(collection.Id, out existing))
			{
				collections.Add(collection.Id, collection);
				existing = collection;
			}
			if (existing.AcceptedTypes.Where(type => type.Id == contentType.Id).Count() == 0)
			{
				existing.AcceptedTypes.Add(contentType);
			}
			if (existing.Categories.Where(cat => cat.Id == category.Id).Count() == 0)
			{
				existing.Categories.Add(category);
			}
			return existing;
		}).Distinct();
}
</pre>
(Figure 7: Aggregation is... less pleasant, but still not terrible.  And there's probably a more elgant way to do it that I don't know)

We're performing a join between the Collection and the entities it aggregates in the above example.  To express this to Dapper, we have to declare each of the resultant types and pass in a function that takes an instance of each of those types as a parameter.  This function is called for each row that Dapper gets as a result of the SQL Query: the passed in entities will already be populated with whatever Dapper was able to infer from the row's data, but it will be up to you to weed out duplicates manually [think about the data that SQL returns when you join multiple tables] and essentially build the final result yourself.  In the end, it expects you to pass back from the function an instance of the root aggregate that you wish returned.

I use a separate Dictionary, declared outside of the function, to track which Collection objects I've already seen to prevent duplication.  If I've seen it before, I use (and will pass back) the instance that I got from this externally held Dictionary.  If not, I add it to the Dictionary and then proceed to use it.  This way, the resultant Collection object will retain and add to its aggregates as new children are found (in this case, AcceptedTypes and Categories).  In the end, I pick only the distinct results to weed out the fact that I returned (and thus was going to have passed back) potentially multiple references to the same Collection object.  There's probably a better way to do this, but it worked well enough for the experiment.

So where from here with Dapper?  It ended up being pretty tedious to lay out all of the infrastructure needed to even make simple Repositories out of it, so I just did a couple examples and left it at that.  Like I said before, it defeats the purpose of Dapper to use it as the backend for more heavy-handed Repository structures, as enlightening as it was to play with it in this manner.  Do consider Dapper if you need to optimize particular queries however: you'll love the raw speed increase it can provide.

